{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "788c45c8",
   "metadata": {},
   "source": [
    "# Advanced PII detection and anonymization with Hugging Face Transformers and Amazon SageMaker\n",
    "\n",
    "In this blog, you will learn how to use state-of-the-art Transformers models to recognize, detect and anonymize PII using Hugging Face Transformers, Presidio & Amazon SageMaker.\n",
    "\n",
    "### What is Presidio?\n",
    "\n",
    "_Presidio (Origin from Latin praesidium â€˜protection, garrisonâ€™) helps to ensure sensitive data is properly managed and governed. It provides fast identification and anonymization modules for private entities in text and images such as credit card numbers, names, locations, social security numbers, bitcoin wallets, US phone numbers, financial data and more._ - [Documentation](https://microsoft.github.io/presidio/).\n",
    "\n",
    "\n",
    "![presidio-gif](assets/presidio.gif)  \n",
    "_- From Presidio [Documentation](https://microsoft.github.io/presidio/)_\n",
    "\n",
    "By Default Presidio is using `Spacy` for PII indentification and extraction. In this example are we going to replace `spacy` with a Hugging Face Transformer to perform PII detection and anonymization.  \n",
    "Presidio supports already out of the box [24 PII entities including](https://microsoft.github.io/presidio/supported_entities/), CREDIT_CARD, IBAN_CODE, EMAIL_ADDRESS, US_BANK_NUMBER, US_ITIN... \n",
    "We are going to extend this availabe 24 entities with transformers to include LOCATION, PERSON & ORGANIZATION. But it is possible to use any \"entity\" extracted by the transformers model. \n",
    "\n",
    "\n",
    "You will learn how to: \n",
    "\n",
    "1. Setup Environment and Permissions\n",
    "2. Create a new `transformers` basedÂ EntityRecognizer\n",
    "3. Create a custom `inference.py` including the `EntityRecognizer`\n",
    "4. Deploy the PII service to Amazon SageMaker\n",
    "5. Request and customization of requests\n",
    "\n",
    "Let's get started! ðŸš€\n",
    "\n",
    "---\n",
    "\n",
    "*If you are going to use Sagemaker in a local environment (not SageMaker Studio or Notebook Instances). You need access to an IAM Role with the required permissions for Sagemaker. You can findÂ [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html)Â more about it.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5237d478",
   "metadata": {},
   "source": [
    "## 1. Setup Environment and Permissions\n",
    "\n",
    "_*Note:* we only install the required libraries from Hugging Face and AWS. You also need PyTorch or Tensorflow, if you havenÂ´t it installed_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c59d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sagemaker --upgrade\n",
    "import sagemaker\n",
    "\n",
    "assert sagemaker.__version__ >= \"2.75.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0ef431",
   "metadata": {},
   "source": [
    "Install `git` and `git-lfs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d8dfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For notebook instances (Amazon Linux)\n",
    "!sudo yum update -y \n",
    "!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.rpm.sh | sudo bash\n",
    "!sudo yum install git-lfs git -y\n",
    "# For other environments (Ubuntu)\n",
    "!sudo apt-get update -y \n",
    "!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\n",
    "!sudo apt-get install git-lfs git -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4386d9",
   "metadata": {},
   "source": [
    "### Permissions\n",
    "\n",
    "_If you are going to use Sagemaker in a local environment (not SageMaker Studio or Notebook Instances). You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c22e8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't call 'get_role' to get Role ARN from role name philippschmid to get Role path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::558105141721:role/sagemaker_execution_role\n",
      "sagemaker bucket: sagemaker-us-east-1-558105141721\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39967284",
   "metadata": {},
   "source": [
    "## 2. Create a new `transformers` basedÂ EntityRecognizer\n",
    "\n",
    "Presidio can be extended to support the detection of new types of PII entities and to support additional languages. These PII recognizers could be addedÂ **via code**Â orÂ **ad-hoc as part of the request**.\n",
    "\n",
    "- TheÂ `EntityRecognizer`Â is an abstract class for all recognizers.\n",
    "- TheÂ `RemoteRecognizer`Â is an abstract class for calling external PII detectors. See more infoÂ [here](https://microsoft.github.io/presidio/analyzer/adding_recognizers/#creating-a-remote-recognizer).\n",
    "- The abstract classÂ `LocalRecognizer`Â is implemented by all recognizers running within the Presidio-analyzer process.\n",
    "- TheÂ `PatternRecognizer`Â is a class for supporting regex and deny-list-based recognition logic, including validation (e.g., with checksum) and context support. See an exampleÂ [here](https://microsoft.github.io/presidio/analyzer/adding_recognizers/#simple-example).\n",
    "\n",
    "For simple recognizers based on regular expressions or deny-lists, we can leverage the providedÂ `PatternRecognizer`:\n",
    "\n",
    "```python\n",
    "from presidio_analyzer import PatternRecognizer\n",
    "titles_recognizer = PatternRecognizer(supported_entity=\"TITLE\",\n",
    "                                      deny_list=[\"Mr.\",\"Mrs.\",\"Miss\"])\n",
    "```\n",
    "\n",
    "To create a Hugging Face Transformer recognizer you have to create a new class deriving the `EntityRecognizer` and implementing a `load` and `analyze` method. \n",
    "\n",
    "For this example the `__init__` method will be used to \"load\" and our model using the `transformers.pipeline` for `token-classification`. \n",
    "If you want to learn more how you can customize/create recognizer you can check out the [documentation](https://microsoft.github.io/presidio/analyzer/adding_recognizers/#extending-the-analyzer-for-additional-pii-entities).\n",
    "\n",
    "\n",
    "```python\n",
    " class TransformersRecognizer(EntityRecognizer):    \n",
    "    def __init__(self,model_id_or_path=None,aggregation_strategy=\"average\",supported_language=\"en\",ignore_labels=[\"O\",\"MISC\"]):\n",
    "      # inits transformers pipeline for given mode or path\n",
    "      self.pipeline = pipeline(\"token-classification\",model=model_id_or_path,aggregation_strategy=\"average\",ignore_labels=ignore_labels)\n",
    "      # map labels to presidio labels\n",
    "      self.label2presidio={\n",
    "        \"PER\": \"PERSON\",\n",
    "        \"LOC\": \"LOCATION\",\n",
    "        \"ORG\": \"ORGANIZATION\",\n",
    "      }\n",
    "\n",
    "      # passes entities from model into parent class\n",
    "      super().__init__(supported_entities=list(self.label2presidio.values()),supported_language=supported_language)\n",
    "\n",
    "    def load(self) -> None:\n",
    "        \"\"\"No loading is required.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def analyze(\n",
    "        self, text: str, entities: List[str]=None, nlp_artifacts: NlpArtifacts=None\n",
    "    ) -> List[RecognizerResult]:\n",
    "        \"\"\"\n",
    "        Extracts entities using Transformers pipeline\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # keep max sequence length in mind\n",
    "        predicted_entities = self.pipeline(text)\n",
    "        if len(predicted_entities) >0:\n",
    "          for e in predicted_entities:\n",
    "            converted_entity = self.label2presidio[e[\"entity_group\"]]\n",
    "            if converted_entity in entities or entities is None:\n",
    "              results.append(\n",
    "                  RecognizerResult(\n",
    "                      entity_type=converted_entity,\n",
    "                      start=e[\"start\"],\n",
    "                      end=e[\"end\"],\n",
    "                      score=e[\"score\"]\n",
    "                      )\n",
    "                  )\n",
    "        return results\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755fd6da",
   "metadata": {},
   "source": [
    "## 3. Create a custom `inference.py` including the `EntityRecognizer`\n",
    "\n",
    "To use the custom inference script, you need to create an `inference.py` script. In this example, we are going to overwrite the `model_fn` to load our `HFTransformersRecognizer` correctly and the `predict_fn` to run the pii analysis.\n",
    "\n",
    "Additionally we need to provide a `requirements.txt` in the `code/` directory to install `presidio` and other required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc01de44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: code: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ce41529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/inference.py\n",
    "\n",
    "from presidio_anonymizer import AnonymizerEngine\n",
    "from presidio_analyzer import AnalyzerEngine\n",
    "from typing import List\n",
    "\n",
    "from presidio_analyzer import AnalyzerEngine, EntityRecognizer, RecognizerResult\n",
    "from presidio_analyzer.nlp_engine import NlpArtifacts\n",
    "from transformers import pipeline\n",
    "\n",
    "# load spacy model -> workaround\n",
    "import os\n",
    "os.system(\"spacy download en_core_web_lg\")\n",
    "\n",
    "# list of entities: https://microsoft.github.io/presidio/supported_entities/#list-of-supported-entities\n",
    "DEFAULT_ANOYNM_ENTITIES = [\n",
    "    \"CREDIT_CARD\",\n",
    "    \"CRYPTO\",\n",
    "    \"DATE_TIME\",\n",
    "    \"EMAIL_ADDRESS\",\n",
    "    \"IBAN_CODE\",\n",
    "    \"IP_ADDRESS\",\n",
    "    \"NRP\",\n",
    "    \"LOCATION\",\n",
    "    \"PERSON\",\n",
    "    \"PHONE_NUMBER\",\n",
    "    \"MEDICAL_LICENSE\",\n",
    "    \"URL\",\n",
    "    \"ORGANIZATION\"\n",
    "]\n",
    "\n",
    "# init anonymize engine\n",
    "engine = AnonymizerEngine()\n",
    "\n",
    "class HFTransformersRecognizer(EntityRecognizer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_id_or_path=None,\n",
    "        aggregation_strategy=\"simple\",\n",
    "        supported_language=\"en\",\n",
    "        ignore_labels=[\"O\", \"MISC\"],\n",
    "    ):\n",
    "        # inits transformers pipeline for given mode or path\n",
    "        self.pipeline = pipeline(\n",
    "            \"token-classification\", model=model_id_or_path, aggregation_strategy=aggregation_strategy, ignore_labels=ignore_labels\n",
    "        )\n",
    "        # map labels to presidio labels\n",
    "        self.label2presidio = {\n",
    "            \"PER\": \"PERSON\",\n",
    "            \"LOC\": \"LOCATION\",\n",
    "            \"ORG\": \"ORGANIZATION\",\n",
    "        }\n",
    "\n",
    "        # passes entities from model into parent class\n",
    "        super().__init__(supported_entities=list(self.label2presidio.values()), supported_language=supported_language)\n",
    "\n",
    "    def load(self) -> None:\n",
    "        \"\"\"No loading is required.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def analyze(\n",
    "        self, text: str, entities: List[str] = None, nlp_artifacts: NlpArtifacts = None\n",
    "    ) -> List[RecognizerResult]:\n",
    "        \"\"\"\n",
    "        Extracts entities using Transformers pipeline\n",
    "        \"\"\"\n",
    "        results = []\n",
    "\n",
    "        # keep max sequence length in mind\n",
    "        predicted_entities = self.pipeline(text)\n",
    "        if len(predicted_entities) > 0:\n",
    "            for e in predicted_entities:\n",
    "                converted_entity = self.label2presidio[e[\"entity_group\"]]\n",
    "                if converted_entity in entities or entities is None:\n",
    "                    results.append(\n",
    "                        RecognizerResult(\n",
    "                            entity_type=converted_entity, start=e[\"start\"], end=e[\"end\"], score=e[\"score\"]\n",
    "                        )\n",
    "                    )\n",
    "        return results\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    transformers_recognizer = HFTransformersRecognizer(model_dir)\n",
    "    # Set up the engine, loads the NLP module (spaCy model by default) and other PII recognizers\n",
    "    analyzer = AnalyzerEngine()\n",
    "    analyzer.registry.add_recognizer(transformers_recognizer)\n",
    "    return analyzer\n",
    "\n",
    "\n",
    "def predict_fn(data, analyzer):\n",
    "    sentences = data.pop(\"inputs\", data)\n",
    "    if \"parameters\" in data:\n",
    "        anonymization_entities = data[\"parameters\"].get(\"entities\", DEFAULT_ANOYNM_ENTITIES)\n",
    "        anonymize_text = data[\"parameters\"].get(\"anonymize\", False)\n",
    "    else:\n",
    "        anonymization_entities = DEFAULT_ANOYNM_ENTITIES\n",
    "        anonymize_text = False\n",
    "\n",
    "    # identify entities\n",
    "    results = analyzer.analyze(text=sentences, entities=anonymization_entities, language=\"en\")\n",
    "    # anonymize text\n",
    "    if anonymize_text:\n",
    "        result = engine.anonymize(text=sentences, analyzer_results=results)\n",
    "        return {\"anonymized\": result.text}\n",
    "\n",
    "    return {\"found\": [entity.to_dict() for entity in results]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "387a2fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/requirements.txt\n",
    "\n",
    "presidio-analyzer\n",
    "spacy\n",
    "transformers\n",
    "presidio-anonymizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144d8ccb",
   "metadata": {},
   "source": [
    "## 4. Deploy the PII service to Amazon SageMaker\n",
    "\n",
    "Before you can deploy a t he pii service to Amazon SageMaker you need to create `model.tar.gz` with inference script and model.\n",
    "You need to bundle the `inference.py` and all model-artifcats, e.g. `pytorch_model.bin` into a `model.tar.gz`. The `inference.py` script will be placed into a `code/` folder. We will use `git` and `git-lfs` to easily download our model from hf.co/models and upload it to Amazon S3 so we can use it when creating our SageMaker endpoint.\n",
    "\n",
    "As the base model for the recognizer the example will use [Jean-Baptiste/roberta-large-ner-english](https://huggingface.co/Jean-Baptiste/roberta-large-ner-english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "952983b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "repository = \"Jean-Baptiste/roberta-large-ner-english\"\n",
    "model_id=repository.split(\"/\")[-1]\n",
    "s3_location=f\"s3://{sess.default_bucket()}/custom_inference/{model_id}/model.tar.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374ff630",
   "metadata": {},
   "source": [
    "1. Download the model from hf.co/models with `git clone`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a134b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git lfs install\n",
    "!git clone https://huggingface.co/$repository\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a6f330",
   "metadata": {},
   "source": [
    "2. copy `inference.py`  into the `code/` directory of the model directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6146af09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/philipp/Projects/personal/blog/advanced-pii-huggingface-sagemaker\n"
     ]
    }
   ],
   "source": [
    "!cp -r code/ $model_id/code/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e1395a",
   "metadata": {},
   "source": [
    "3. Create a `model.tar.gz` archive with all the model artifacts and the `inference.py` script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcfda24",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd $model_id\n",
    "!tar zcvf model.tar.gz *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c858560",
   "metadata": {},
   "source": [
    "4. Upload the `model.tar.gz` to Amazon S3:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dc7ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp model.tar.gz $s3_location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a146346",
   "metadata": {},
   "source": [
    "After you uploaded the `model.tar.gz` archive to Amazon S3. You can create a custom `HuggingfaceModel` class. This class will be used to create and deploy our SageMaker endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a271a2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "   model_data=s3_location,       # path to your model and script\n",
    "   role=role,                    # iam role with permissions to create an Endpoint\n",
    "   transformers_version=\"4.17\",  # transformers version used\n",
    "   pytorch_version=\"1.10\",        # pytorch version used\n",
    "   py_version='py38',            # python version used\n",
    ")\n",
    "\n",
    "# deploy the endpoint endpoint\n",
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g4dn.xlarge\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b3812f",
   "metadata": {},
   "source": [
    "## 5. Request and customization of requests\n",
    "\n",
    "The `.deploy()` returns an `HuggingFacePredictor` object which can be used to request inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45f06083",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload=\"\"\"\n",
    "Hello, my name is David Johnson and I live in Maine.\n",
    "I work as a software engineer at Amazon. \n",
    "You can call me at (123) 456-7890.\n",
    "My credit card number is 4095-2609-9393-4932 and my crypto wallet id is 16Yeky6GMjeNkAiNcBY7ZhrLoMSgg1BoyZ.\n",
    "\n",
    "On September 18 I visited microsoft.com and sent an email to test@presidio.site, from the IP 192.168.0.1.\n",
    "My passport: 191280342 and my phone number: (212) 555-1234.\n",
    "This is a valid International Bank Account Number: IL150120690000003111111. Can you please check the status on bank account 954567876544?\n",
    "Kate's social security number is 078-05-1126.  Her driver license? it is 1234567A.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a0baca",
   "metadata": {},
   "source": [
    "**Simple detection request**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dfdc9a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'found': [{'entity_type': 'CREDIT_CARD', 'start': 120, 'end': 139, 'score': 1.0, 'analysis_explanation': None, 'recognition_metadata': {'recognizer_name': 'CreditCardRecognizer'}}, {'entity_type': 'CRYPTO', 'start': 167, 'end': 201, 'score': 1.0, 'analysis_explanation': None, 'recognition_metadata': {'recognizer_name': 'CryptoRecognizer'}}, {'entity_type': 'EMAIL_ADDRESS', 'start': 265, 'end': 283, 'score': 1.0, 'analysis_explanation': None, 'recognition_metadata': {'recognizer_name': 'EmailRecognizer'}}, {'entity_type': 'IBAN_CODE', 'start': 421, 'end': 444, 'score': 1.0, 'analysis_explanation': None, 'recognition_metadata': {'recognizer_name': 'IbanRecognizer'}}, {'entity_type': 'PERSON', 'start': 19, 'end': 32, 'score': 0.9997117519378662, 'analysis_explanation': None, 'recognition_metadata': {'recognizer_name': 'HFTransformersRecognizer'}}, {'entity_type': 'LOCATION', 'start': 47, 'end': 52, 'score': 0.9993120431900024, 'analysis_explanation': None, 'recognition_metadata': {'recognizer_name': 'HFTransformersRecognizer'}}, {'entity_type': 'PERSON', 'start': 508, 'end': 512, 'score': 0.9965325593948364, 'analysis_explanation': None, 'recognition_metadata': {'recognizer_name': 'HFTransformersRecognizer'}}, {'entity_type': 'ORGANIZATION', 'start': 87, 'end': 93, 'score': 0.9888795614242554, 'analysis_explanation': None, 'recognition_metadata': {'recognizer_name': 'HFTransformersRecognizer'}}, {'entity_type': 'IP_ADDRESS', 'start': 297, 'end': 308, 'score': 0.95, 'analysis_explanation': None, 'recognition_metadata': {'recognizer_name': 'IpRecognizer'}}, {'entity_type': 'DATE_TIME', 'start': 207, 'end': 219, 'score': 0.85, 'analysis_explanation': None, 'recognition_metadata': {'recognizer_name': 'SpacyRecognizer'}}, {'entity_type': 'PHONE_NUMBER', 'start': 354, 'end': 368, 'score': 0.75, 'analysis_explanation': None, 'recognition_metadata': {'recognizer_name': 'PhoneRecognizer'}}, {'entity_type': 'PHONE_NUMBER', 'start': 541, 'end': 552, 'score': 0.75, 'analysis_explanation': None, 'recognition_metadata': {'recognizer_name': 'PhoneRecognizer'}}, {'entity_type': 'ORGANIZATION', 'start': 230, 'end': 239, 'score': 0.5814294815063477, 'analysis_explanation': None, 'recognition_metadata': {'recognizer_name': 'HFTransformersRecognizer'}}, {'entity_type': 'ORGANIZATION', 'start': 274, 'end': 276, 'score': 0.5579692721366882, 'analysis_explanation': None, 'recognition_metadata': {'recognizer_name': 'HFTransformersRecognizer'}}, {'entity_type': 'URL', 'start': 230, 'end': 243, 'score': 0.5, 'analysis_explanation': None, 'recognition_metadata': {'recognizer_name': 'UrlRecognizer'}}, {'entity_type': 'URL', 'start': 270, 'end': 281, 'score': 0.5, 'analysis_explanation': None, 'recognition_metadata': {'recognizer_name': 'UrlRecognizer'}}]}\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "  \"inputs\": payload,\n",
    "}\n",
    "\n",
    "res = predictor.predict(data=data)\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dede63",
   "metadata": {},
   "source": [
    "**Detect only specific PII entities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "049a66cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'found': [{'entity_type': 'PERSON', 'start': 19, 'end': 32, 'score': 0.9997117519378662, 'analysis_explanation': None, 'recognition_metadata': {'recognizer_name': 'HFTransformersRecognizer'}}, {'entity_type': 'LOCATION', 'start': 47, 'end': 52, 'score': 0.9993120431900024, 'analysis_explanation': None, 'recognition_metadata': {'recognizer_name': 'HFTransformersRecognizer'}}, {'entity_type': 'PERSON', 'start': 508, 'end': 512, 'score': 0.9965325593948364, 'analysis_explanation': None, 'recognition_metadata': {'recognizer_name': 'HFTransformersRecognizer'}}, {'entity_type': 'ORGANIZATION', 'start': 87, 'end': 93, 'score': 0.9888795614242554, 'analysis_explanation': None, 'recognition_metadata': {'recognizer_name': 'HFTransformersRecognizer'}}, {'entity_type': 'ORGANIZATION', 'start': 230, 'end': 239, 'score': 0.5814294815063477, 'analysis_explanation': None, 'recognition_metadata': {'recognizer_name': 'HFTransformersRecognizer'}}, {'entity_type': 'ORGANIZATION', 'start': 274, 'end': 276, 'score': 0.5579692721366882, 'analysis_explanation': None, 'recognition_metadata': {'recognizer_name': 'HFTransformersRecognizer'}}]}\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "  \"inputs\": payload,\n",
    "  \"parameters\": {\n",
    "    \"entities\":[\"PERSON\",\"LOCATION\",\"ORGANIZATION\"]\n",
    "  }\n",
    "}\n",
    "\n",
    "res = predictor.predict(data=data)\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494962a2",
   "metadata": {},
   "source": [
    "**Anonzymizing PII entities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b265224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hello, my name is <PERSON> and I live in <LOCATION>.\n",
      "I work as a software engineer at <ORGANIZATION>. \n",
      "You can call me at <PHONE_NUMBER>.\n",
      "My credit card number is <CREDIT_CARD> and my crypto wallet id is <CRYPTO>.\n",
      "\n",
      "On <DATE_TIME> I visited <URL> and sent an email to <EMAIL_ADDRESS>, from the IP <IP_ADDRESS>.\n",
      "My passport: 191280342 and my phone number: <PHONE_NUMBER>.\n",
      "This is a valid International Bank Account Number: <IBAN_CODE>. Can you please check the status on bank account 954567876544?\n",
      "<PERSON>'s social security number is <PHONE_NUMBER>.  Her driver license? it is 1234567A.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "  \"inputs\": payload,\n",
    "  \"parameters\": {\n",
    "    \"anonymize\": True,\n",
    "  }\n",
    "}\n",
    "\n",
    "res = predictor.predict(data=data)\n",
    "print(res[\"anonymized\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5db30a1",
   "metadata": {},
   "source": [
    "**Anonzymizing only specific PII entities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eae2fb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hello, my name is <PERSON> and I live in <LOCATION>.\n",
      "I work as a software engineer at Amazon.\n",
      "My credit card number is 4095-2609-9393-4932 and my crypto wallet id is 16Yeky6GMjeNkAiNcBY7ZhrLoMSgg1BoyZ.\n",
      "\n",
      "On September 18 I visited microsoft.com and sent an email to test@presidio.site, from the IP 192.168.0.1.\n",
      "My passport: 191280342 and my phone number: (212) 555-1234.\n",
      "This is a valid International Bank Account Number: IL150120690000003111111. Can you please check the status on bank account 954567876544?\n",
      "<PERSON>'s social security number is 078-05-1126.  Her driver license? it is 1234567A.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "  \"inputs\": payload,\n",
    "  \"parameters\": {\n",
    "    \"anonymize\": True,\n",
    "    \"entities\":[\"PERSON\",\"LOCATION\"]\n",
    "  }\n",
    "}\n",
    "\n",
    "res = predictor.predict(data=data)\n",
    "print(res[\"anonymized\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb10007d",
   "metadata": {},
   "source": [
    "### Delete model and endpoint\n",
    "\n",
    "To clean up, we can delete the model and endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1e6fb7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a90e25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5fcf248a74081676ead7e77f54b2c239ba2921b952f7cbcdbbe5427323165924"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('hf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
